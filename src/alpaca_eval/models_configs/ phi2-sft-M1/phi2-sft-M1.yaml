# ---
# _name_or_path: microsoft/phi-2
# architectures:
# - PhiForCausalLM
# attention_dropout: 0
# auto_map:
#   AutoConfig: microsoft/phi-2--configuration_phi.PhiConfig
#   AutoModelForCausalLM: microsoft/phi-2--modeling_phi.PhiForCausalLM
# bos_token_id: 50256
# embd_pdrop: 0
# eos_token_id: 50256
# hidden_act: gelu_new
# hidden_size: 2560
# initializer_range: 0.02
# intermediate_size: 10240
# layer_norm_eps: 1.0e-05
# max_position_embeddings: 2048
# model_type: phi
# num_attention_heads: 32
# num_hidden_layers: 32
# num_key_value_heads: 32
# partial_rotary_factor: 0.4
# qk_layernorm: false
# resid_pdrop: 0.1
# rope_scaling: 
# rope_theta: 10000
# tie_word_embeddings: false
# torch_dtype: bfloat16
# transformers_version: 4.40.0
# use_cache: true
# vocab_size: 51200

 phi2-sft-M1: # this should be the same as the name as the current directory
  prompt_template: "alpaca-7b/prompt.txt" # what prompt should be used for this model
  fn_completions: "huggingface_local_completions" # what function should be used to generate completions. See `src/alpaca_eval/decoders` for options
  completions_kwargs: # parameters to the completion function
    model_name: "Dudep/phi2-sft-M1"
    model_kwargs:
      torch_dtype: 'bfloat16'
      trust_remote_code: True
    max_new_tokens: 2048
    temperature: 0.7
    top_p: 1.0
    do_sample: True
  pretty_name: "phi2-sft-M1" # name in the leaderboard
  link: "https://huggingface.co/Dudep/phi2-sft-M1" # link to the model's repo/information in the leaderboard

